#@title CNN Model

class CNN_Model():
  def segmentor(self,input1):
    #encoder
    if K.image_data_format() == 'channels_last':
      bn_axis = 3
    else:
      bn_axis = 1

    n = ZeroPadding2D(padding=(3, 3), name='Conv1_pad')(input1)
    x = Conv2D(64, (7, 7), strides=(2, 2), padding='valid', kernel_initializer='he_normal', name='conv1')(n)
    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)
    x = ReLU(name='FRelu')(x)
    #print('self.sl1',np.shape(self.sl1))
    x = ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)
    x = MaxPooling2D((3, 3), strides=(2, 2))(x)

    x = self.conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))
    x = self.identity_block(x, 3, [64, 64, 256], stage=2, block='b')
    #print('self.sl2',np.shape(self.sl2))
    x = self.identity_block(x, 3, [64, 64, 256], stage=2, block='c')
    #G1=GlobalAveragePooling2D(name='avg_pool_1')(x)

    x = self.conv_block(x, 3, [128, 128, 512], stage=3, block='a')
    #print('self.sl3',np.shape(self.sl3))
    x = self.identity_block(x, 3, [128, 128, 512], stage=3, block='b')
    x = self.identity_block(x, 3, [128, 128, 512], stage=3, block='c')
    #print('self.add2',(np.shape(self.add2)))
    x = self.identity_block(x, 3, [128, 128, 512], stage=3, block='d')
    #G2=GlobalAveragePooling2D(name='avg_pool_2')(x)

    x = self.conv_block(x, 3, [256, 256, 1024], stage=4, block='a')
    #print('self.sl4',np.shape(self.sl4))
    x = self.identity_block(x, 3, [256, 256, 1024], stage=4, block='b')
    x = self.identity_block(x, 3, [256, 256, 1024], stage=4, block='c')
    x = self.identity_block(x, 3, [256, 256, 1024], stage=4, block='d')
    x = self.identity_block(x, 3, [256, 256, 1024], stage=4, block='e')
    x = self.identity_block(x, 3, [256, 256, 1024], stage=4, block='f')
    #G3=GlobalAveragePooling2D(name='avg_pool_3')(x)

    x = self.conv_block(x, 3, [512, 512, 2048], stage=5, block='a')
    x = self.identity_block(x, 3, [512, 512, 2048], stage=5, block='b')
    self.last = self.identity_block(x, 3, [512, 512, 2048], stage=5, block='c')
    #G4=GlobalAveragePooling2D(name='avg_pool_4')(x)
    self.encoder=Model(inputs=input1,outputs=self.last)
    self.encoder.load_weights='/content/drive/MyDrive/datasets/Pro Can/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'
    #self.encoder.summary()
    print('Encoder build')

    self.sl1=self.encoder.get_layer('FRelu').output
    self.sl2=self.encoder.get_layer('convres2astage2relu').output
    self.sl3=self.encoder.get_layer('convres3astage1relu').output
    self.sl4=self.encoder.get_layer('convres4astage1relu').output
    #self.last=self.encoder.get_layer('convres2astage2relu').output
    self.bttleneck=self.encoder.get_layer('convres5astage1relu').output

    self.add1=self.sl2
    self.add2=self.encoder.get_layer("ires3bstage3relu").output
    self.add3=self.encoder.get_layer("convres4astage3relu").output
    self.add4=self.last
    #print('add4',np.shape(self.add4))
    #self.add4=self.encoder.get_layer("ires3bstage3relu").output
    #self.add4=self.encoder.get_layer("conv5_block2_out").output

    #Decoder
    d1=self.decoder(self.bttleneck,self.sl4,(1,1),(4,4),256)  #self.Dconv(d1,10,50)
    self.D1=Conv2D(filters=3, kernel_size=(1, 1), activation='softmax',dilation_rate=(1,1), name='D1')(self.Dconv(d1,2,256,4))

    d2=self.decoder(d1,self.sl3,(1,1),(2,2),128) #self.Dconv(d2,8,60)
    self.D2=Conv2D(3, kernel_size=(1, 1), activation='softmax',dilation_rate=(1,1),name='D2')(self.Dconv(d2,2,128,3))

    d3=self.decoder(d2,self.sl2,(1,1),(1,1),64) #self.Dconv(d3,10,40)
    self.D3=Conv2D(3, kernel_size=(1, 1), activation='softmax',dilation_rate=(1,1),name='D3')(self.Dconv(d3,2,64,2))

    d4=self.decoder(d3,self.sl1,(1,1),(1,1),8) #self.Dconv(d4,9,30)
    self.outputs=Conv2D(3, kernel_size=(1, 1), activation='softmax',dilation_rate=(1,1),name='Final')(self.Dconv(d4,2,16,1))

    #self.outputs=Conv2D(2,3,padding='same',activation='softmax',dilation_rate=(1,1),name='Final')(self.Dconv(d4,2,8,1))
    #self.seg_model = Model(input1,[self.D1,self.D2,self.D3,self.D4,self.outputs])

    self.seg_model = Model(input1,[self.D1,self.D2,self.D3,self.outputs])
    #print(self.seg_model.summary())
    print('seg_model build')
    return self.seg_model

  def logloss(self,y_true,y_pred):
    #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)
    # clip to prevent NaN's and Inf's
    y_pred = K.clip(y_pred,K.epsilon(), 1 - K.epsilon())
    #weights=np.array([0.5,0.125,0.125,0.125,0.125])
    #weights = K.variable(weights)
    loss = y_true * K.log(y_pred)
    loss = K.mean(-K.sum(loss, -1))
    return loss

  def Intermediate_loss(self,a,b):
    L=tf.keras.losses.categorical_crossentropy(a,b)
    #print(tf.math.reduce_sum(tf.cast(keras.losses.categorical_crossentropy(a[:,:,0],b[:,:,0]),tf.int32),[0,1,2]),tf.math.reduce_sum(tf.cast(self.dice_coef(a[:,:,0],b[:,:,0]),tf.int32))
    #L=K.sum(tf.cast(tf.keras.losses.categorical_crossentropy(a[:,:,:],b[:,:,:]),tf.float32),0)+K.sum(tf.cast(self.dice_coef(a[:,:,:],b[:,:,:]),tf.float32))
    return L

  def Decoder(self,x, nb_filters, strides,i):
    res_path = BatchNormalization()(x)
    res_path = LeakyReLU(alpha=0.2)(res_path)
    res_path = Conv2D(filters=nb_filters, kernel_size=(3, 3), padding='same', dilation_rate=i, strides=strides, kernel_regularizer=regularizers.l2(1e-5))(res_path)
    res_path = BatchNormalization()(res_path)
    res_path = LeakyReLU(alpha=0.2)(res_path)
    res_path = Conv2D(filters=nb_filters, kernel_size=(3, 3), padding='same', dilation_rate=i, strides=strides, kernel_regularizer=regularizers.l2(1e-5))(res_path)
    shortcut = Conv2D(nb_filters, kernel_size=(1, 1), strides=strides, dilation_rate=i, kernel_regularizer=regularizers.l2(1e-5))(x)
    shortcut = BatchNormalization()(shortcut)
    res_path = Add()([shortcut, res_path])
    return res_path


  def LW(self):
    lossWeights = {"self.D1": 0.125, "self.D2": 0.125, "self.D3": 0.125,  "self.outputs": 0.5}


  def Dconv(self,layer,q,filters,n):
    for i in range(n):
      layer=Conv2DTranspose(filters, (q, q), strides=(2, 2), padding='same',dilation_rate=(1,1))(layer)
      filters=filters/2
    return layer



  def build(self):
    #print(self.model.summary)
    return self.seg_model

  def load(self,path):
    self.seg_model.load_weights(path)
    return self.seg_model


  def conv_blck(self,inputs, num_filters):
    x=Conv2D(num_filters,3,padding='same')(inputs)
    x=BatchNormalization()(x)
    x=Activation('relu')(x)

    x=Conv2D(num_filters,3,padding='same')(inputs)
    x=BatchNormalization()(x)
    x=Activation('relu')(x)
    return x

  def decoder(self,inputs, skip_features,strides, i,num_filters):
    x=Conv2DTranspose(num_filters,(2,2),strides=2,padding='same')(inputs)
    x=self.attention(x,skip_features,num_filters)
    x=Concatenate()([x,skip_features])
    x=self.Decoder(x, num_filters, strides,i)
    return x


  def conv_block(self,input_tensor, kernel_size,filters,stage,block,strides=(2, 2)):
    eps = 1.1e-5
    conv_name_base ='res_' + str(stage) + block + 'Clas_branch'
    bn_name_base = 'bn_' + str(stage) + block + 'Clas_branch'
    if K.image_data_format() == 'channels_last':
      bn_axis = 3
    else:
      bn_axis = 1

    filters1, filters2, filters3 = filters
    x = Conv2D(filters1, (1, 1), strides=strides,
                      kernel_initializer='he_normal',
                      name=conv_name_base + '2a')(input_tensor)
    x = BatchNormalization(epsilon=eps,axis=bn_axis, name=bn_name_base + 'BN2a')(x)
    x = ReLU(name='convres' + str(stage) + block + 'stage1relu')(x)

    #x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
    x = Conv2D(filters2, kernel_size, padding='same',
                      kernel_initializer='he_normal',
                      name=conv_name_base + '2b')(x)
    x = BatchNormalization(epsilon=eps,axis=bn_axis, name=bn_name_base + 'BN2b')(x)
    x = ReLU(name='convres' + str(stage) + block + 'stage2relu')(x)

    x = Conv2D(filters3, (1, 1),
                      kernel_initializer='he_normal',
                      name=conv_name_base + '2c')(x)
    x = BatchNormalization(epsilon=eps,axis=bn_axis, name=bn_name_base + 'BN2c')(x)

    shortcut = Conv2D(filters3, (1, 1), strides=strides,
                             kernel_initializer='he_normal',
                             name=conv_name_base + '1')(input_tensor)
    shortcut = BatchNormalization(epsilon=eps,axis=bn_axis, name=bn_name_base + '1')(shortcut)

    x = Add()([x, shortcut])
    x = ReLU(name='convres' + str(stage) + block + 'stage3relu')(x)
    return x

  def identity_block(self,input_tensor, kernel_size, filters, stage, block):
    if K.image_data_format() == 'channels_last':
        bn_axis = 3
    else:
        bn_axis = 1
    eps = 1.1e-5
    filters1, filters2, filters3 = filters
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'

    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)
    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
    x = ReLU(name='ires' + str(stage) + block + 'stage1relu')(x)

    #x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
    x = Conv2D(filters2, kernel_size,
               padding='same', name=conv_name_base + '2b')(x)
    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
    x = ReLU(name='ires' + str(stage) + block + 'stage2relu')(x)

    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)
    x = BatchNormalization(epsilon=eps,axis=bn_axis, name=bn_name_base + '2c')(x)

    x = Add()([x, input_tensor])
    x = ReLU(name='ires' + str(stage) + block + 'stage3relu')(x)
    return x


  def dice_coef(self,y_true, y_pred):
    y_true_f = K.flatten(tf.cast(y_true,tf.float32))
    y_pred_f = K.flatten(tf.cast(y_pred,tf.float32))
    intersection = tf.math.multiply(y_true_f , y_pred_f)
    return 1-((2 * intersection + 1) / (K.sum(y_true_f)+K.sum(y_pred_f) + 1))

  def custom_cse(self,y_true,y_pred):
    labels = tf.cast(K.flatten(tf.convert_to_tensor([y_true])),tf.uint)
    probs = K.flatten(tf.convert_to_tensor([y_pred]))
    loss = K.sum(labels * (-tf.math.log(probs)) + (1 - labels) * (-tf.math.log(1 - probs)))
    return loss

  def attention(self,x, g,num_filters):
    x=Conv2D(num_filters,1,strides=1,padding='same')(x)
    g=Conv2D(num_filters,1,strides=1,padding='same')(g)
    A=Add()([x,g])
    R=ReLU()(A)
    C=Conv2D(1,1,strides=1,padding='same')(R)
    S=tf.keras.activations.sigmoid(C)
    X=UpSampling2D(size=(1,1))(S)
    X=Conv2D(num_filters,1,strides=1,padding='same')(X)
    F=tf.keras.layers.multiply([x,X])
    F=BatchNormalization()(F)
    return F

  def dice_loss(self,y_true, y_pred):
    return 1-self.dice_score(y_true, y_pred)

  def dice_coef(self,y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return 1-((2 * intersection + 0.0001) / (K.sum(y_true_f) + K.sum(y_pred_f) + 0.0001))

  def dice_coef_0(self,y_true, y_pred,smooth=0.000001):
    y_true_f = K.flatten(y_true[:,:,:,0])
    y_pred_f = K.flatten(y_pred[:,:,:,0])
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
    #return tf.keras.losses.categorical_crossentropy(y_true[:,:,:,0],y_pred[:,:,:,0])

  def dice_coef_1(self,y_true, y_pred,smooth=0.000001):
      y_true_f = K.flatten(y_true[:,:,:,1])
      y_pred_f = K.flatten(y_pred[:,:,:,1])
      intersection = K.sum(y_true_f * y_pred_f)
      return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
      #return tf.keras.losses.categorical_crossentropy(y_true[:,:,:,1],y_pred[:,:,:,2])

  def dice_coef_2(self,y_true, y_pred,smooth=0.000001):
      y_true_f = K.flatten(y_true[:,:,:,2])
      y_pred_f = K.flatten(y_pred[:,:,:,2])
      intersection = K.sum(y_true_f * y_pred_f)
      return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
      #return tf.keras.losses.categorical_crossentropy(y_true[:,:,:,2],y_pred[:,:,:,2])

  def dice_score(self,y_true, y_pred):
      d0 = self.dice_coef_0(y_true, y_pred,smooth=0.000001)
      d1 = self.dice_coef_1(y_true, y_pred,smooth=0.000001)
      d2 = self.dice_coef_2(y_true, y_pred,smooth=0.000001)
      dice_mean = (d0+d1+d2)/3
      return dice_mean

  def weighted_dice_score(self,y_true, y_pred):
    ds0 = self.dice_coef_0(y_true, y_pred,smooth=0.000001)
    ds1 = self.dice_coef_1(y_true, y_pred,smooth=0.000001)
    ds2 = self.dice_coef_2(y_true, y_pred,smooth=0.000001)

    weighted_dice_mean = (ds0+ds1+ds2)/3
    return weighted_dice_mean

  def weighted_dice_loss(self,y_true, y_pred):
    return 1-self.weighted_dice_score(y_true, y_pred)

  def gen_dice_loss(self,y_true, y_pred):
    #return tf.keras.losses.categorical_crossentropy(y_true,y_pred)
    return self.weighted_dice_loss(y_true, y_pred)+tf.keras.losses.categorical_crossentropy(y_true,y_pred)

  def train(self):
    losses = {
        'D1':self.gen_dice_loss,
        'D2':self.gen_dice_loss,
        'D3':self.gen_dice_loss,
        #'D4':self.gen_dice_loss,
        'Final':self.gen_dice_loss}
    lossWeights = {"Final": 0.5, "D1": 0.125, "D2": 0.125, "D3": 0.125}

    imgloader=img_loader()
    step_p_ep=50
    checkpoint_path = "/content/drive/MyDrive/datasets/RINGS dataset/withoutAttention{epoch:02d}.h5"
    cp_callback = [
                 #tf.keras.callbacks.LearningRateScheduler(scheduler),
                 tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True,verbose=1),
                 tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15),
                 tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,patience=3, min_lr=0.000000000000000000000001),
                 tf.keras.callbacks.TensorBoard(log_dir="./logs")
                 ]
    self.seg_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                           loss=losses,
                           metrics=[self.dice_score,tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])
    history=self.seg_model.fit((imgloader.imageLoader(mode=1)),validation_data=imgloader.imageLoader(mode=2),validation_steps=30,epochs=90,steps_per_epoch=50,callbacks=[cp_callback])

  def load_train(self,path_img,path_mask,path):
    imgloader=img_loader()
    step_p_ep=100
    checkpoint_path = "/content/drive/MyDrive/datasets/Pro Can/weights/training_1/Pro{epoch:02d}.h5"
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)
    self.seg_model.load_weights(path)
    #self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[segmentation_models.metrics.IOUScore(0.5),keras.metrics.Precision(),keras.metrics.Recall()])
    history=self.seg_model.fit((imgloader.imageLoader(path_img,path_mask)),epochs=10,steps_per_epoch=step_p_ep/1,callbacks=[cp_callback])
    return history

